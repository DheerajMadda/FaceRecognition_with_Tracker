{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f746bd7e",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098f4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from mtcnn import MTCNN\n",
    "# insightface folder\n",
    "from insightface.src.common import face_preprocess\n",
    "from insightface.deploy import face_model\n",
    "\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22d7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train_dir\n",
    "train_dir = os.path.join(os.getcwd(),'train')\n",
    "\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "    \n",
    "# Create embedding_dir\n",
    "embedding_dir = os.path.join(os.getcwd(),'faceEmbeddingModels')\n",
    "if not os.path.exists(embedding_dir):\n",
    "    os.mkdir(embedding_dir)\n",
    "    \n",
    "# Detector = mtcnn_detector\n",
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0c556",
   "metadata": {},
   "source": [
    "# 1. Collect User Image for Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = '112,112'\n",
    "max_images = 20\n",
    "user = input()\n",
    "\n",
    "def collectImagesFromCamera(user, max_images):\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    faces = 0\n",
    "    frames = 0\n",
    "    max_faces = max_images\n",
    "    max_bbox = np.zeros(4)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(os.path.join(train_dir, user)):\n",
    "        os.makedirs(os.path.join(train_dir, user))\n",
    "        \n",
    "    while faces < max_faces:\n",
    "        ret, frame = cap.read()\n",
    "        frames += 1\n",
    "        time_string = str(int(time.time()))\n",
    "        \n",
    "        # Get all faces on current frame\n",
    "        bboxes = detector.detect_faces(frame)\n",
    "        \n",
    "        if len(bboxes) != 0:\n",
    "            # Get only the biggest face\n",
    "            max_area = 0\n",
    "            for bboxe in bboxes:\n",
    "                bbox = bboxe[\"box\"]\n",
    "                bbox = np.array([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "                keypoints = bboxe[\"keypoints\"]\n",
    "                area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
    "                if area > max_area:\n",
    "                    max_bbox = bbox\n",
    "                    landmarks = keypoints\n",
    "                    max_area = area\n",
    "                    \n",
    "            max_bbox = max_bbox[0:4]\n",
    "            \n",
    "            # get each of 3 frames\n",
    "            if frames % 3 == 0:\n",
    "                # convert to face_preprocess.preprocess input\n",
    "                landmarks = np.array([landmarks[\"left_eye\"][0], landmarks[\"right_eye\"][0], landmarks[\"nose\"][0],\n",
    "                                      landmarks[\"mouth_left\"][0], landmarks[\"mouth_right\"][0],\n",
    "                                      landmarks[\"left_eye\"][1], landmarks[\"right_eye\"][1], landmarks[\"nose\"][1],\n",
    "                                      landmarks[\"mouth_left\"][1], landmarks[\"mouth_right\"][1]])\n",
    "                landmarks = landmarks.reshape((2, 5)).T\n",
    "                nimg = face_preprocess.preprocess(frame, max_bbox, landmarks, image_size= image_size)\n",
    "\n",
    "                cv2.imwrite(os.path.join(os.path.join(train_dir, user), \"{}.jpg\".format(time_string)), nimg)\n",
    "                cv2.rectangle(frame, (max_bbox[0], max_bbox[1]), (max_bbox[2], max_bbox[3]), (255, 0, 0), 2)\n",
    "                faces += 1\n",
    "                print(\"[INFO] {} Image Captured\".format(faces))\n",
    "                    \n",
    "        \n",
    "        cv2.imshow(\"Face detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "collectImagesFromCamera(user, max_images)\n",
    "\n",
    "'''\n",
    "# SAMPLE OUTPUT of detector.detect_faces(frame) :-\n",
    "[\n",
    "    {\n",
    "        'box': [277, 90, 48, 63],\n",
    "        'keypoints':\n",
    "        {\n",
    "            'nose': (303, 131),\n",
    "            'mouth_right': (313, 141),\n",
    "            'right_eye': (314, 114),\n",
    "            'left_eye': (291, 117),\n",
    "            'mouth_left': (296, 143)\n",
    "        },\n",
    "        'confidence': 0.99851983785629272\n",
    "    }\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83469ff3",
   "metadata": {},
   "source": [
    "# 2. Generate Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369c6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = '112,112'\n",
    "model = \"./insightface/models/model-y1-test2/model,0\"\n",
    "threshold = 1.24\n",
    "det = 0\n",
    "embeddings_file = \"embeddings.pickle\"\n",
    "\n",
    "def genFaceEmbedding():\n",
    "    # Grab the paths to the input images in our dataset\n",
    "    imagePaths = list(paths.list_images(train_dir))\n",
    "    # Initialize the faces embedder\n",
    "    embedding_model = face_model.FaceModel(image_size, model, threshold, det)\n",
    "    \n",
    "    # Initialize our lists of extracted facial embeddings and corresponding people names\n",
    "    knownEmbeddings = []\n",
    "    knownNames = []\n",
    "\n",
    "    # Initialize the total number of faces processed\n",
    "    total = 0\n",
    "\n",
    "    # Loop over the imagePaths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extract the person name from the image path\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "        # load the image\n",
    "        image = cv2.imread(imagePath)\n",
    "        # convert face to RGB color\n",
    "        nimg = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        nimg = np.transpose(nimg, (2, 0, 1))\n",
    "        # Get the face embedding vector\n",
    "        face_embedding = embedding_model.get_feature(nimg)\n",
    "\n",
    "        # add the name of the person + corresponding face\n",
    "        # embedding to their respective list\n",
    "        knownNames.append(name)\n",
    "        knownEmbeddings.append(face_embedding)\n",
    "        total += 1\n",
    "        \n",
    "    print(total, \" faces embedded\")\n",
    "\n",
    "    # save to output\n",
    "    data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "    with open(os.path.join(embedding_dir,embeddings_file), \"wb\") as f:\n",
    "        f.write(pickle.dumps(data))\n",
    "\n",
    "genFaceEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b34f7",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "754071d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the face embeddings\n",
    "data = pickle.loads(open(os.path.join(embedding_dir,embeddings_file), \"rb\").read())\n",
    "embeddings = np.array(data[\"embeddings\"])\n",
    "input_shape = embeddings.shape[1]  # required for creating model\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data[\"names\"])\n",
    "num_classes = len(np.unique(labels)) # required for creating model\n",
    "\n",
    "labels = labels.reshape(-1, 1)\n",
    "ct = ColumnTransformer([(\"names\", OneHotEncoder(), [0])], remainder = 'passthrough')\n",
    "labels = ct.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1e653",
   "metadata": {},
   "source": [
    "# 4. Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1024940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(input_shape, num_classes):\n",
    "    \n",
    "    # Build sofmax classifier\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "my_model = createModel(input_shape, num_classes)\n",
    "    \n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "my_model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3f5d8",
   "metadata": {},
   "source": [
    "# 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7efb2131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 5s 263ms/step - loss: 0.5905 - accuracy: 0.7667 - val_loss: 0.2476 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.1820 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.3571e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1.5520e-04 - accuracy: 1.0000 - val_loss: 1.6048e-05 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 1.3143e-05 - accuracy: 1.0000 - val_loss: 2.0713e-06 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 3.8408e-06 - accuracy: 1.0000 - val_loss: 5.9605e-07 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 1.2070e-06 - accuracy: 1.0000 - val_loss: 2.3842e-07 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 3.8743e-07 - accuracy: 1.0000 - val_loss: 1.1921e-07 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 2.4587e-07 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 1.7509e-07 - accuracy: 1.0000 - val_loss: 1.4901e-07 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 8.5682e-08 - accuracy: 1.0000 - val_loss: 1.1921e-07 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 1.1176e-07 - accuracy: 1.0000 - val_loss: 1.0431e-07 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 8.9407e-08 - accuracy: 1.0000 - val_loss: 1.0431e-07 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 1.0803e-07 - accuracy: 1.0000 - val_loss: 7.4506e-08 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 7.4506e-08 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1.2293e-07 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 8.5682e-08 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 6.7055e-08 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 1.7136e-07 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 7.0780e-08 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 7.8231e-08 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 8.1956e-08 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1.1548e-07 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 1.1548e-07 - accuracy: 1.0000 - val_loss: 5.9605e-08 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
    "# Create KFold\n",
    "cv = KFold(n_splits = 5, random_state = 42, shuffle=True)\n",
    "\n",
    "def trainModel():\n",
    "\n",
    "    # Train\n",
    "    for train_idx, valid_idx in cv.split(embeddings):\n",
    "        X_train, X_val, y_train, y_val = embeddings[train_idx], embeddings[valid_idx], labels[train_idx], labels[valid_idx]\n",
    "        his = my_model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(X_val, y_val))\n",
    "\n",
    "        history['accuracy'] += his.history['accuracy']\n",
    "        history['val_accuracy'] += his.history['val_accuracy']\n",
    "        history['loss'] += his.history['loss']\n",
    "        history['val_loss'] += his.history['val_loss']\n",
    "        \n",
    "trainModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daebef9",
   "metadata": {},
   "source": [
    "# 6. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a2ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.save('my_model.h5')\n",
    "\n",
    "# also save the lables\n",
    "labels_file = \"le.pickle\"\n",
    "with open(os.path.join(embedding_dir,labels_file), \"wb\") as f:\n",
    "    f.write(pickle.dumps(le))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2f69f",
   "metadata": {},
   "source": [
    "# 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "632bc6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ./insightface/models/model-y1-test2/model 0\n"
     ]
    }
   ],
   "source": [
    "image_size = '112,112'\n",
    "model = \"./insightface/models/model-y1-test2/model,0\"\n",
    "threshold = 1.24\n",
    "det = 0\n",
    "\n",
    "# Initialize faces embedding model\n",
    "embedding_model = face_model.FaceModel(image_size, model, threshold, det)\n",
    "\n",
    "embeddings = os.path.join(embedding_dir, embeddings_file)\n",
    "le = os.path.join(embedding_dir, labels_file)\n",
    "\n",
    "# Load embeddings and labels\n",
    "data = pickle.loads(open(embeddings, \"rb\").read())\n",
    "le = pickle.loads(open(le, \"rb\").read())\n",
    "\n",
    "embeddings = np.array(data['embeddings'])\n",
    "labels = le.fit_transform(data['names'])\n",
    "\n",
    "# Load the classifier model\n",
    "my_model = tensorflow.keras.models.load_model('./my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1654f6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 : 480\n",
      "800 : 600\n",
      "Recognized: dheeraj <100.00>\n",
      "Recognized: dheeraj <100.00>\n"
     ]
    }
   ],
   "source": [
    "def findCosineDistance(vector1, vector2):\n",
    "        \"\"\"\n",
    "        Calculate cosine distance between two vector\n",
    "        \"\"\"\n",
    "        vec1 = vector1.flatten()\n",
    "        vec2 = vector2.flatten()\n",
    "\n",
    "        a = np.dot(vec1.T, vec2)\n",
    "        b = np.dot(vec1.T, vec1)\n",
    "        c = np.dot(vec2.T, vec2)\n",
    "        return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
    "\n",
    "def CosineSimilarity(test_vec, source_vecs):\n",
    "        \"\"\"\n",
    "        Verify the similarity of one vector to group vectors of one class\n",
    "        \"\"\"\n",
    "        cos_dist = 0\n",
    "        for source_vec in source_vecs:\n",
    "            cos_dist += findCosineDistance(test_vec, source_vec)\n",
    "        return cos_dist / len(source_vecs)\n",
    "\n",
    "def detectface():\n",
    "    # Initialize some useful arguments\n",
    "    cosine_threshold = 0.8   # similarity threshold\n",
    "    proba_threshold = 0.85    # predicted threshold/confidence\n",
    "    comparing_num = 5\n",
    "    # Tracker params\n",
    "    trackers = []\n",
    "    texts = []\n",
    "    frames = 0\n",
    "    \n",
    "    # Start streaming and recording\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    print(str(frame_width) + \" : \" + str(frame_height))\n",
    "    save_width = 800\n",
    "    save_height = int(800 / frame_width * frame_height)\n",
    "    print(str(save_width) + \" : \" + str(save_height))\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        frames += 1\n",
    "        \n",
    "        if frame is not None:\n",
    "            frame = cv2.resize(frame, (save_width, save_height))\n",
    "        \n",
    "            if frames % 3 == 0:\n",
    "                trackers = []\n",
    "                texts = []\n",
    "\n",
    "                bboxes =  detector.detect_faces(frame)\n",
    "            \n",
    "                if len(bboxes) != 0:\n",
    "\n",
    "                    for bboxe in bboxes:\n",
    "                        bbox = bboxe['box']\n",
    "                        bbox = np.array([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "                        landmarks = bboxe['keypoints']\n",
    "                        landmarks = np.array([landmarks[\"left_eye\"][0], landmarks[\"right_eye\"][0], landmarks[\"nose\"][0],\n",
    "                                              landmarks[\"mouth_left\"][0], landmarks[\"mouth_right\"][0],\n",
    "                                              landmarks[\"left_eye\"][1], landmarks[\"right_eye\"][1], landmarks[\"nose\"][1],\n",
    "                                              landmarks[\"mouth_left\"][1], landmarks[\"mouth_right\"][1]])\n",
    "                        landmarks = landmarks.reshape((2, 5)).T\n",
    "                        nimg = face_preprocess.preprocess(frame, bbox, landmarks, image_size='112,112')\n",
    "                        nimg = cv2.cvtColor(nimg, cv2.COLOR_BGR2RGB)\n",
    "                        nimg = np.transpose(nimg, (2, 0, 1))\n",
    "                        embedding = embedding_model.get_feature(nimg).reshape(1, -1)\n",
    "\n",
    "                        text = \"Unknown\"\n",
    "                    \n",
    "                        # Predict class\n",
    "                        preds = my_model.predict(embedding)\n",
    "                        preds = preds.flatten()\n",
    "                        # Get the highest accuracy embedded vector\n",
    "                        j = np.argmax(preds)\n",
    "                        proba = preds[j]\n",
    "                    \n",
    "                        # Compare this vector to source class vectors to verify it is actual belong to this class\n",
    "                        match_class_idx = (labels == j)\n",
    "                        match_class_idx = np.where(match_class_idx)[0]\n",
    "                        selected_idx = np.random.choice(match_class_idx, comparing_num)\n",
    "                        compare_embeddings = embeddings[selected_idx]\n",
    "                    \n",
    "                        # Calculate cosine similarity\n",
    "                        cos_similarity = CosineSimilarity(embedding, compare_embeddings)\n",
    "                    \n",
    "                        if cos_similarity < cosine_threshold and proba > proba_threshold:\n",
    "                            name =  le.classes_[j]\n",
    "                            text = \"{}\".format(name)\n",
    "                            print(\"Recognized: {} <{:.2f}>\".format(name, proba * 100))\n",
    "                        \n",
    "                        # Start tracking\n",
    "                        tracker = dlib.correlation_tracker()\n",
    "                        rect = dlib.rectangle(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "                        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        tracker.start_track(rgb, rect)\n",
    "                        trackers.append(tracker)\n",
    "                        texts.append(text)\n",
    "                    \n",
    "                        y = bbox[1] - 10 if bbox[1] - 10 > 10 else bbox[1] + 10\n",
    "                        cv2.putText(frame, text, (bbox[0], y), cv2.FONT_HERSHEY_SIMPLEX, 0.95, (255, 255, 255), 1)\n",
    "                        cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (179, 0, 149), 4)\n",
    "                    \n",
    "            else:\n",
    "                for tracker, text in zip(trackers, texts):\n",
    "                    pos = tracker.get_position()\n",
    "\n",
    "                    # unpack the position object\n",
    "                    startX = int(pos.left())\n",
    "                    startY = int(pos.top())\n",
    "                    endX = int(pos.right())\n",
    "                    endY = int(pos.bottom())\n",
    "\n",
    "                    cv2.putText(frame, text, (startX, startY - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.95, (255, 255, 255), 1)\n",
    "                    cv2.rectangle(frame, (startX, startY), (endX, endY), (179, 0, 149), 4)\n",
    "        \n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            # frame = None\n",
    "            pass\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "             \n",
    "detectface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4748e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
